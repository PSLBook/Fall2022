--- 
title: "Practical Statistical Learning"
author: "John Marden and Feng Liang"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
# url: your book url like https://bookdown.org/yihui/bookdown
# cover-image: path to the social sharing image like images/cover.jpg
description: |
  This is a minimal example of using the bookdown package to write a book.
  The HTML output format for this example is bookdown::gitbook,
  set in the _output.yml file.
link-citations: yes
github-repo: rstudio/bookdown-demo
---
--- 
title: "Practical Statistical Learning"
author: "John Marden and Feng Liang"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
# url: your book url like https://bookdown.org/yihui/bookdown
# cover-image: path to the social sharing image like images/cover.jpg
description: |
  This is a minimal example of using the bookdown package to write a book.
  The HTML output format for this example is bookdown::gitbook,
  set in the _output.yml file.
link-citations: yes
github-repo: rstudio/bookdown-demo
---

# Preface{-}

These notes are based on a course in statistical learning developed by John Marden and Feng Liang at UIUC using the text **The Elements of Statistical Learning** by Hastie, Tibshirani and Friedman [https://hastie.su.domains/ElemStatLearn/](https://hastie.su.domains/ElemStatLearn/). 


<!--chapter:end:index.Rmd-->


# Introduction {#intro}

Placeholder


## Introduction to Statistical Learning
### Types of learning problems 
### Challenge of supervised learning 
### Curse of dimensionality 
### A glimpse of learning theory 
### Bias and variance tradeoff 
##  Least squares vs. nearest neighbors
### Introduction to LS and kNN 
### Simulation study with R
### Simulation study with Python
### Compute Bayes rule 
### Discussion 

<!--chapter:end:01-intro.Rmd-->


# Linear Regression {#linear-model-chapter}

Placeholder


## Good predictions: Squared error loss and in-sample error {#good-pred-section}
## Matrices and least-squares estimates {#matrix-section}
## Regression inference
## Geometric interpretation 
###  Basic concepts in vector spaces
###  LS and projection
## In-sample prediction
## Practical issues
### Using R
### Interpret LS coefficients
### Handle categorical variables
### Outliers and rank deficiency

<!--chapter:end:02-linear-regression.Rmd-->

# Variable Selection and Regularization

**Notes**
[[lec_W3_VariableSelection.pdf](https://liangfgithub.github.io/Notes/lec_W3_VariableSelection.pdf)]
[[lec_W3_PCR.pdf](https://liangfgithub.github.io/Notes/lec_W3_PCR.pdf)]
[[lec_W3_More_on_Ridge_Lasso.pdf](https://liangfgithub.github.io/Notes/lec_W3_More_on_Ridge_Lasso.pdf)]

**R/Python Code**
[[Rcode_W3_VarSel_SubsetSelection](https://liangfgithub.github.io/Rcode_W3_VarSel_SubsetSelection.html)]
[[Rcode_W3_VarSel_RidgeLasso](https://liangfgithub.github.io/Rcode_W3_VarSel_RidgeLasso.html)]

## Subset selection


## Ridge regression

## Lasso regression

<!--chapter:end:03-regularization.Rmd-->

# Regression Trees

<!--chapter:end:04-regression-trees.Rmd-->

# Nonlinear Regression

Chapter 2 assumed that the mean of the dependent variables was a linear function of the explanatory variables. In this chapter we will consider non-linear functions. We start with just one $x$-variable, and consider the model
\begin{equation}
Y_i = f(x_i)+e_i,~~i=1,\ldots,n,
(\#eq:smooth1)
\end{equation}
where the $x_i$'s are given, and the $e_i$'s are independent with mean zero and variances $\sigma^2_e$. A linear model would have $f(x_i)=\beta_0+\beta_1x_i$. Here, we are not constraining $f$ to be linear, or even any parametric function. Basically, $f$ can be any function as long as it is sufficiently **smooth**. Exactly what we mean by smooth will be detailed later. 

From a prediction point of view, the goal is to find an estimated function of $f$, $\widehat f$, so that new $y$'s can be predicted from new $x$'s by $\widehat f(x)$. Related but not identical goals include

- **Curve-fitting**: fit a smooth curve to the data in order to have a good summary of the data; find a curve so that the graph "looks nice";

- **Interpolation**: Estimate $y$ for values of $x$ not among the observed, but in the  same range as the observed;

- **Extrapolation**: Estimate $y$ for values of $x$ outside the range of the observed $x$'s, a somewhat dangerous activity.

This chapter deals with **nonparametric** functions $f$, which strictly speaking means that we are not assuming a particular form of the function based on a finite number of parameters. Examples of parametric nonlinear functions:
\begin{equation}
f(x) = \alpha~e^{\beta x}~~\text{and}~~f(x) =\sin(\alpha+\beta~x+\gamma x^2).
(\#eq:smooth2)
\end{equation}
Such models can be fit with least squares much as the linear models, although the derivatives are not simple linear functions of the parameters, and Newton-Raphson or some other numerical method is needed.

The approach we take to estimating $f$ in the nonparametric model is to use some sort of **basis expansion** of the functions on $\mathbb{R}$. That is, we have an infinite set of known functions, $h_1(x), h_2(x),\ldots,$ and estimate $f$ using a linear combination of a subset of the functions, e.g.,
\begin{equation}
\widehat f(x) = \widehat{\beta}_0+\widehat{\beta}_1 h_1(x) +\cdots+\widehat{\beta}_m h_m(x).
(\#eq:smooth3)
\end{equation}

We are **not** assuming that $f$ is a finite linear combination of the $h_j$'s, hence will always have a biased estimator of $f$. Usually we *do* assume that $f$ can be arbitrarily well approximated by such a linear combination, that is, there is a sequence $\beta_0,\beta_1,\beta_2,\ldots,$ such that
\begin{equation}
f(x) = \beta_0+\lim_{m\to\infty}\sum_{i=1}^m \beta_j h_j(x)
(\#eq:smooth4)
\end{equation}
uniformly, at least for $x$ in a finite range.

An advantage to using estimates as in \@ref(eq:smooth3) is that the estimated function is a linear one, linear in the $h_j$'s, though not in the $x$. But with $x_i$'s fixed, we are in the same estimation bailiwick as the linear models in Chapter 2, hence ideas such as subset selection, ridge, lasso, and estimating prediction errors carry over reasonably easily.

In the next few sections, we consider possible sets of $h_j$'s, including polynomials and local polynomials such as cubic splines. 


## Polynomials

The estimate of $f$ is a polynomial in $x$, where the challenge is to figure out the degree. In raw form, we have
\begin{equation}
h_1(x) = x,~h_2(x) = x^2,~h_3(x) = x^3, \ldots.
(\#eq:poly1)
\end{equation}
(The Weierstrass Approximation Theorem guarantees that \@ref(eq:smooth4) holds.) 
The $m^{th}$ degree polynomial fit is then
\begin{equation}
\widehat f(x) = 
\widehat\beta_0+ \widehat\beta_1x+\widehat\beta_2x^2+\cdots+\widehat\beta_mx^m.
(\#eq:poly2)
\end{equation}
It is straightforward to find the estimates $\widehat\beta_j$ using the techniques from Chapter 2, where here
\begin{equation}
\uX = \begin{pmatrix}
1&x_1&x_1^2&\cdots&x_1^m\\
1&x_2&x_2^2&\cdots&x_2^m\\
\vdots&\vdots&\vdots&\ddots&\vdots\\
1&x_N&x_N^2&\cdots&x_N^m
\end{pmatrix}.
(\#eq:poly3)
\end{equation}

Technically, one could perform a regular subset regression procedure, but generally one considers only fits allowing the
first $m$ coefficients to be nonzero, and requiring the rest to be zero. Thus the only fits considered are
\begin{equation}
\begin{array}{rclc}
\widehat f(x_i) &=& \widehat{\beta}_0&\text{(constant)}\\
\widehat f(x_i) &=& \widehat\beta_0+ \widehat\beta_1x_i&\text{(linear)}\\
\widehat f(x_i) &=& \widehat\beta_0+ \widehat\beta_1x_i+ \widehat\beta_2x_i^2&\text{(quadratic)}\\
&\vdots&\\
\widehat f(x_i) &=& \widehat\beta_0+ \widehat\beta_1x_i+\widehat\beta_2x_i^2+\cdots+\widehat\beta_mx_i^m
&\text{($m^{th}$ degree)}\\
&\vdots&
\end{array}
(\#eq:poly4)
\end{equation}


We will use the birthrate data to illustrate polynomial fits. The $x$'s are the years from 1917 to 2003, and the
$y$'s are the births per 10,000 women aged twenty-three in the 
U.S.^[The data up through 1975 can be found in the Data and Story Library at
http://lib.stat.cmu.edu/DASL/Datafiles/Birthrates.html. See
Velleman, P. F. and Hoaglin, D. C. (1981). *Applications, Basics, and Computing of 
Exploratory Data Analysis*. Belmont. CA: Wadsworth, Inc. The original data is from
P.K. Whelpton and A. A. Campbell, "Fertility Tables for Birth Charts of American Women," Vital Statistics Special Reports 51, no. 1. 
(Washington D.C.:Government Printing Office, 1960, years 1917-1957)
and National Center for Health Statistics, Vital Statistics of the United States Vol. 1, Natality (Washington D.C.:Government Printing Office, yearly, 1958-1975).
The data from 1976 to 2003 are actually rates for women
aged 20-24, found in 
the National Vital Statistics Reports
Volume 54, Number 2 September 8, 2005, *Births: Final Data for 2003*;
http://www.cdc.gov/nchs/data/nvsr/nvsr54/nvsr54\_02.pdf.]


Figure \@ref(fig:poly-br-fig) contains the fits of several polynomials, from a cubic to an $80^{th}$-degree polynomial.
It looks like the $m=3$ and $m=5$ fits are poor, $m=20$ to 40 are reasonable, and $m=80$ is overfitting, i.e., the
curve is too jagged.


### Model selection

### Using R


The $\mathbf{X}$ matrix in \@ref(eq:poly3) is not the one to use for computations. For any high-degree polynomial, we will end up with huge numbers (e.g., $87^{16}\approx 10^{31}$) and small numbers in the matrix, which leads to numerical inaccuracies. A better matrix uses *orthogonal polynomials*, or in fact orthonormal ones. Thus we want the columns of the $\mathbf{X}$ matrix, except for the intercept column $\mathbf{1}_n$, to have mean 0 and norm 1, but also to have them orthogonal to each other in such a way that the first $m$ columns still yield the $m^{th}$-degree polynomials. To illustrate, without going into much detail, we use the Gram-Schmidt algorithm for $\mathbf{x}=(1,2,3,4,5)'$ and $m=2$. Start with the raw columns,
\begin{equation}
\uOne_5=\begin{pmatrix}1\\1\\1\\1\\1\end{pmatrix},
\mathbf{x}_{[1]} = \begin{pmatrix}1\\2\\3\\4\\5\end{pmatrix},~~\text{and}~~
\mathbf{x}_{[2]} = \begin{pmatrix}1\\4\\9\\16\\25\end{pmatrix}.
(\#eq:poly10)
\end{equation}
Let the first one as is, but subtract the means (3 and 11) from each of the other two:
\begin{equation}
{\mathbf{x}}^{(2)}_{[1]} = \begin{pmatrix}-2\\-1\\0\\1\\2\end{pmatrix},~~\text{and}~~
{\mathbf{x}}^{(2)}_{[2]} = \begin{pmatrix}-10\\-7\\-2\\5\\14\end{pmatrix}.
(\#eq:poly11)
\end{equation}
Now leave the first two alone, and make the third orthogonal to the second by applying the main Gram-Schmidt step,
\begin{equation}
\uu \to \uu - {\uu'\uv\over\uv'\uv}~\uv,
(\#eq:poly12)
\end{equation}
with $\uv={\mathbf{x}}^{(2)}_{[1]}$ and $\uu={\mathbf{x}}^{(2)}_{[2]}$:
\begin{equation}
\mathbf{x}_{[2]}^{[3]} = 
\begin{pmatrix}-10\\-7\\-2\\5\\14\end{pmatrix} - {60\over 10}~\begin{pmatrix}-2\\-1\\0\\1\\2\end{pmatrix}
= \begin{pmatrix}
2\\  -1\\  -2\\ -1\\ 2\end{pmatrix}.
(\#eq:poly13)
\end{equation}
To complete the picture, divide the last two $x$'s by their respective norms, to get
\begin{equation}
\uOne_5=\begin{pmatrix}1\\1\\1\\1\\1\end{pmatrix},
\mathbf{x}^{Norm}_{[1]} = {1\over \sqrt{10}}~\begin{pmatrix}-2\\-1\\0\\1\\2\end{pmatrix},~~\text{and}~~
{\mathbf{x}}^{Norm}_{[2]} ={1\over\sqrt{14}} \begin{pmatrix}-10\\-7\\-2\\5\\14\end{pmatrix}.
(\#eq:poly14)
\end{equation}
You can check that indeed these three vectors are orthogonal, and the last two orthonormal.

For a large $N$ and $m$, you would continue, at step $k$ orthogonalizing the current $(k+1)^{st},\ldots,(m+1)^{st}$ 
vector to the current $k^{th}$ vector. Once you have these vectors, then the fitting is easy, because the $\mathbf{X}_m$ for
the $m^{th}$ degree polynomial (leaving out the $\uOne_N$) just uses the first $m$ vectors, and
$\uX_m'\uX_m=\uI_m$, so that the estimates of beta are just $\uX'_m\uy$, and the $\uH_m=\uX_m\uX_m'$. Using the
saturated model, i.e., $(N-1)^{st}$-degree, we can get all the coefficients at once,
\begin{equation}
\widehat{\ubeta} = \uX_{N-1}'\uy,~~(\widehat\beta_0=\overline y).
(\#eq:poly15)
\end{equation}
Then the coefficients for the $m^{th}$ order fit are the first $m$ elements of $\widehat{\ubeta}$. Also, the
residual sum of squares equals the sum of squares of the left-out coefficients:
\begin{equation}
RSS^m = \sum_{j=m+1}^{N-1} \widehat\beta_j^2,
(\#eq:poly16)
\end{equation}
from which it is easy to find the residual variances, $\overline {err}^m$'s, and estimated prediction errors.

The following commands in $R$ will read in the data and find the estimated coefficients and predicted $y$'s.
```
source("http://www.stat.uiuc.edu/~jimarden/birthrates.txt")

N <- 87
x <- 1:87
y <- birthrates[,2]

xx <- poly(x,86)

betah <- t(xx)%*%y

yhats <- sweep(xx,2,betah,"*")
yhats <- t(apply(yhats,1,cumsum)) + mean(y)
# yhats[,m] has the fitted y's for the m-th degree polynomial.
```

Plot the $16^{th}$-degree fit:
```
m <- 16
plot(birthrates,main=paste("m =",m))
lines(x+1916,yhats[,m])
```
You can plot all the fits sequentially using
```
for(m in 1:86) {
   plot(birthrates,main=paste("m =",m))
   lines(x+1916,yhats[,m])
   readline()
}
```
where you hit the enter key to see the next one.


Unfortunately, polynomials are not very good for extrapolation. Using the two polynomial fits, we have the
following extrapolations.



## Regression Splines

## Smoothing Splines

## Sinces and cosines

## A glimpse of wavelets

<!--chapter:end:05-nonlinear-regression.Rmd-->

# Clustering Analysis

<!--chapter:end:06-clustering.Rmd-->

# Latent Structure Models

<!--chapter:end:07-latent-structure-models.Rmd-->

# More on Clustering

<!--chapter:end:08-more-on-clustering.Rmd-->

# Distriminant Analysis

<!--chapter:end:09-distriminant-analysis.Rmd-->

# Logistic Regression

<!--chapter:end:10-logistic-regression.Rmd-->

# Support Vector Machines

<!--chapter:end:11-SVM.Rmd-->

# Classification Trees and Boosting

<!--chapter:end:12-classification-trees.Rmd-->

# Recommender System

<!--chapter:end:13-recommender.Rmd-->

# Introduction to Deep Learning

<!--chapter:end:14-introduction-DL.Rmd-->

