# Linear Regression {#linear-model-chapter}

To ease into machine learning, we start with regular linear models. There is one dependent variable, the $y$, and $p$ explanatory variables, the $x$'s. The data, or training sample, consists of $n$ independent observations: 
$$
(y_1,\mathbf{x}_1), (y_2,\mathbf{x}_2),\ldots,(y_n,\mathbf{x}_n).
$$
For individual $i$, $y_i$ is the value of the  one-dimensional dependent variable, and $\mathbf{x}_i = (x_{i1}, x_{i2}, \cdots, x_{ip})^t$ is the $p\times 1$ vector of values for the explanatory variables. Generally, the $y_i$'s are continuous, but the $x_{ij}$'s can be anything numerical, e.g., 0-1 indicator variables, or functions of another variable (e.g., $x,x^2,x^3$).

The linear model is
\begin{equation}
y_i = \beta_0+\beta_1 x_{i1}+\cdots+\beta_p x_{ip}+e_i.
(\#eq:lm3)
\end{equation}
The $\beta_j$'s are parameters, usually unknown and to be estimated. The $e_i$'s are the errors or residuals. We will assume that

- The $e_i$'s are independent (of each other, and of the $\mathbf{x}_i$'s);
- $\mathbb{E} [e_i]=0$ for each $i$;
- $\text{Var}[e_i] = \sigma^2_e$ for each $i$.

There is also a good chance we will assume they are normally distributed.

From STAT424 and 425 (or other courses), you know what to do now: estimate the $\beta_j$'s and $\sigma^2_e$, decide which $\beta_j$'s are significant, do $F$-tests, look for outliers and other violations of the assumptions, etc.

Here, we may do much of that, but with the goal of prediction. Suppose $(y^*,\mathbf{x}^*)$ is a new point, satisfying the same model and assumptions as above (in particular, being independent of the observed $\mathbf{x}_i$'s). Once we have the estimates of the $\beta_j$'s (based on the observed data), we **predict** $y^*$ from $\mathbf{x}^*$ by
\begin{equation}
\widehat{y}^* = \widehat{\beta}_0+\widehat\beta_1 x^*_1+\cdots+\widehat\beta_p x^{*}_p.
(\#eq:lm4)
\end{equation}
The prediction is good if $\widehat{y}^{*}$ is close to $y^{*}$. We do not know $y^{*}$, but we can hope. But the key point is

> The estimates of the parameters are good if they give good predictions. We don't care if the $\widehat\beta_j$'s are
close to the $\beta_j$'s; we don't care about unbiasedness or minimum variance or significance. We just care whether
we get good predictions.


## Good predictions: Squared error loss and in-sample error {#good-pred-section}

We want the predictions to be close to the actual (unobserved) value of the dependent variable, that is, we want $\widehat{y}^{*}$ close to $y^{*}$. One way to measure closeness is by using squared error:
$$
(y^{*}-\widehat{y}^{*})^2.
$$
Because we do not know $y^{*}$ (yet), we might look at the expected value instead:
$$
E[(Y^{*}-\widehat{Y}^{*})^2].
$$
But what is that the expected value over? Certainly $Y^{*}$, but the $Y_i$'s and $\mathbf{X}_i$'s in the sample, as well as the $\mathbf{X}^{*}$, could all be considered random. There is no universal answer, but for our purposes here we will
assume that the all features $\mathbf{X}$  are fixed, and all the $Y_i$'s are random, i.e., 
\begin{equation}
E[(Y^{*}-\widehat Y^{*})^2 \mid \mathbf{X}_1=\mathbf{x}_1,\ldots,\mathbf{X}_n=\mathbf{x}_n,\mathbf{X}^{*}=\mathbf{x}^{*}].
(\#eq:lm7)
\end{equation}

But typically you are creating a predictor for many new $x$'s, and likely you do not know what they will be. (You don't know what the next 1000 e-mails you get will be.) A reasonable approach is to assume the new $\mathbf{x}$'s will look much like the old ones, hence you would look at the errors for $n$ new $\mathbf{x}_i$'s being the same as the old ones. Thus we would have $n$ new cases, $(y_i^{*},\mathbf{x}_i^{*})$, but where $\mathbf{X}_i^{*}=\mathbf{x}_i$. The $n$ expected errors are averaged, to obtain what is called the **in-sample error**:
\begin{equation}
\text{ERR}_{\text{in}} = \frac{1}{n} \sum_{i=1}^n E[(Y_i^{*}-\widehat Y_i^{*})^2~|~\mathbf{X}_1=\mathbf{x}_1,\ldots,\mathbf{X}_n=\mathbf{x}_n,\mathbf{X}_i^{*}=\mathbf{x}_i^{*}].
(\#eq:lm8)
\end{equation}
In particular situations, you may have a more precise knowledge of what the new $x$'s would be. By all means, use those values.

We will drop the conditional part of the notation for simplicity.


## Matrices and least-squares estimates {#matrix-section}

Ultimately we want to find estimates of the parameters that yield a low \text{ERR}_{\text{in}}. We'll start with the least squares estimate, then translate things to matrices. The estimates of the $\beta_j$'s depends on just the training sample. The
**least squares** estimate of the parameters are the $\beta_j$'s that minimize the objective function
\begin{equation}
\text{RSS}(\beta_0,\ldots,\beta_p) = \sum_{i=1}^n (y_i-\beta_0- \beta_1 x_{i1}-\cdots- \beta_p x_{ip})^2.
(\#eq:lm9)
\end{equation}
The function is a nice convex function in the $\beta_j$'s, so setting the derivatives equal to zero and solving will
yield the minimum. The derivatives are
\begin{align}
{\partial\over\partial \beta_0} \text{RSS} (\beta_0,\ldots,\beta_p) &= -2~\sum_{i=1}^N (y_i-\beta_0-\beta_1 x_{i1}-\cdots-\beta_p x_{ip});\\
{\partial\over\partial b_j} \text{RSS} (\beta_0,\ldots,\beta_p) &= -2~\sum_{i=1}^N x_{ij}(y_i-\beta_0-\beta_1 x_{i1}-\cdots-\beta_p x_{ip}), \quad j\ge1.
(\#eq:lm10)
\end{align}

Write the equations in matrix form, starting with
\begin{equation}
\begin{pmatrix}
y_1-\beta_0-\beta_1 x_{11}-\cdots-\beta_p x_{1p}\\
y_2-\beta_0-\beta_1 x_{21}-\cdots-\beta_p x_{2p}\\
\vdots\\
y_N-\beta_0-\beta_1 x_{N1}-\cdots-\beta_p x_{Np}
\end{pmatrix} =
\begin{pmatrix}
y_1\\y_2\\\vdots\\y_N
\end{pmatrix} - 
\begin{pmatrix}
1&x_{11}&x_{12}&\cdots&x_{1p}\\
1&x_{21}&x_{22}&\cdots&x_{2p}\\
\vdots&\vdots&\vdots&\ddots&\vdots\\
1&x_{N1}&x_{N2}&\cdots&x_{Np}
\end{pmatrix}
\begin{pmatrix}
\beta_0\\\beta_1\\\vdots\\\beta_p
\end{pmatrix} 
(\#eq:lm11)
\end{equation}
which is equal to $\mathbf{y} - \mathbf{X}\boldsymbol{\beta}$. The $n$-by-$(p+1)$ matrix $\mathbf{X}$ is the so-called design matrix. 

Take the two summations in equations \@ref(eq:lm10) (without the $-2$'s) and set to 0 to get
\begin{align}
\begin{pmatrix}1&1&\cdots&1\end{pmatrix}(\mathbf{y}-\mathbf{x}\boldsymbol{\beta})&=0;\\
\begin{pmatrix}x_{1j}&x_{2j}&\cdots&x_{Nj}\end{pmatrix}(\mathbf{y}-\mathbf{x}\boldsymbol{\beta})&=0, \quad j\ge 1.
(\#eq:lm12)
\end{align}
Note that the row vectors in \@ref(eq:lm12) on the left are the $(p+1)$ columns of $\mathbf{X}$, yielding
\begin{equation}
\mathbf{X}^t(\mathbf{y}-\mathbf{X}\boldsymbol{\beta}) = 0.
(\#eq:lm13)
\end{equation}
That equation is easily solved:
\begin{equation}
\mathbf{X}^t\mathbf{y} = \mathbf{X}^t\mathbf{x}\boldsymbol{\beta}~~\Rightarrow~~\boldsymbol{\beta} = (\mathbf{X}^t\mathbf{X})^{-1}\mathbf{X}^t\mathbf{y},
(\#eq:lm14)
\end{equation}
at least if $\mathbf{X}'\mathbf{X}$ is invertible. If it is not invertible, then there will be many solutions. In practice, one can always eliminate some (appropriate) columns of $\mathbf{X}$ to obtain invertibility. Generalized inverses are available, too.

**Summary.** In the linear model
\begin{equation}
\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \mathbf{e},
(\#eq:lm15)
\end{equation}
where 
\begin{equation}
\boldsymbol{\beta} = 
\begin{pmatrix}
\beta_0\\\beta_1\\\vdots\\\beta_p
\end{pmatrix} ~~\text{and}~~
\mathbf{e} = 
\begin{pmatrix}
e_1\\e_2\\\vdots\\e_N
\end{pmatrix},
(\#eq:lm16)
\end{equation}
the least squares estimate of $\boldsymbol{\beta}$, assuming $\mathbf{X}^t\mathbf{X}$ is invertible, is
\begin{equation}
\widehat{\boldsymbol{\beta}}_{LS} = (\mathbf{X}^t\mathbf{X})^{-1}\mathbf{X}^t\mathbf{y}.
(\#eq:lm17)
\end{equation}


## Regression inference

## Geometric interpretation 
###  Basic concepts in vector spaces
###  LS and projection




## In-sample prediction

When considering the in-sample error for the linear model, we have the same model for the training sample and the new sample:
\begin{equation}
\mathbf{Y}=\mathbf{X}\boldsymbol{\beta}+\mathbf{e}~~\mbox{and}~~\mathbf{Y}^{*} = \mathbf{X}\boldsymbol{\beta}+\mathbf{e}^{*}.
(\#eq:predls1)
\end{equation}
The $e_i$'s and $e^{*}_i$'s are independent with mean 0 and variance $\sigma^2_e$. If we use the least-squares estimate of $\boldsymbol{\beta}$ in the prediction, we have
\begin{equation}
\widehat{\mathbf{Y}}^{*} = \mathbf{X}\widehat{\boldsymbol{\beta}}_{LS} = \mathbf{X}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{Y} = \mathbf{H}\mathbf{Y},
(\#eq:predls2)
\end{equation}
where $\mathbf{H}$ is the "hat" matrix,
\begin{equation} 
\mathbf{H} = \mathbf{X}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'.
(\#eq:H)
\end{equation}
Note that this matrix is symmetric and idempotent, which means that
\begin{equation}
\mathbf{H}^t = \mathbf{H}, \quad 
\mathbf{H}\mathbf{H}=\mathbf{H}.
(\#eq:predls3-1)
\end{equation}


The errors in prediction are the $Y_i^{*} - \widehat Y_i^{*}$. Before getting to the $\text{ERR}_{\text{in}}$, consider the mean and covariance's of these errors. First,
\begin{equation}
E[\mathbf{Y}] = E[\mathbf{X}\boldsymbol{\beta}+\mathbf{e}] = \mathbf{X}\boldsymbol{\beta},~~E[\mathbf{Y}^{*}] = E[\mathbf{X}\boldsymbol{\beta}+\mathbf{e}^{*}] = \mathbf{X}\boldsymbol{\beta},
(\#eq:predls4)
\end{equation}
because the expected values of the $e$'s are all 0 and we are assuming $\mathbf{X}$ is fixed, and
\begin{align}
E[\widehat {\mathbf{Y}}^{*}] &= E[\mathbf{X}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{Y}]\\
&= \mathbf{X}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'E[\mathbf{Y}]\\ 
&= \mathbf{X}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{X}\boldsymbol{\beta}\\ 
&= \mathbf{X}\boldsymbol{\beta},
(\#eq:predls4-1)
\end{align}
because the $\mathbf{X}'\mathbf{X}$'s cancel. Thus,
\begin{equation}
\mathbb{E}[\mathbf{Y}^{*} - \widehat {\mathbf{Y}}^{*}] = \mathbf{0}_n~~\mbox{(the $n\times 1$ vector of 0's)}.
(\#eq:predls5)
\end{equation}
This zero means that the errors are **unbiased**. They may be big or small, but on average right on the nose. Unbiasedness is ok, but it is really more important to be close.

Next, the covariance matrices:
\begin{equation}
\text{Cov}[\mathbf{Y}] = \text{Cov}[\mathbf{X}\boldsymbol{\beta}+\mathbf{e}] = \text{Cov}[\mathbf{e}] = \sigma_e^2\mathbf{I}_n~~\mbox{(the $n\times n$ identity matrix)},
(\#eq:predls6)
\end{equation}
because the $e_i$'s are independent, hence have zero covariance, and all have variance $\sigma^2_e$.
Similarly,
\begin{equation}
\text{Cov}[\mathbf{Y}^{*}] = \sigma_e^2\mathbf{I}_n.
(\#eq:predls7)
\end{equation}
Less similar,
\begin{align}
\text{Cov}[\widehat {\mathbf{Y}}^{*}] &= \text{Cov}[\mathbf{X}(\mathbf{X}^t\mathbf{X})^{-1}\mathbf{X}^t\mathbf{Y}]\\
&=\mathbf{X}(\mathbf{X}^t\mathbf{X})^{-1}\mathbf{X}^t \text{Cov}[\mathbf{Y}] \mathbf{X}(\mathbf{X}^t\mathbf{X})^{-1}\mathbf{X}^t\\
&=\mathbf{X}(\mathbf{X}^t\mathbf{X})^{-1}\mathbf{X}^t \sigma^2_e\mathbf{I}_n \mathbf{X}(\mathbf{X}^t\mathbf{X})^{-1}\mathbf{X}^t\\
&=\sigma^2_e\mathbf{X}(\mathbf{X}^t\mathbf{X})^{-1}\mathbf{X}^t\mathbf{X}(\mathbf{X}^t\mathbf{X})^{-1}\mathbf{X}^t\\
&=\sigma^2_e\mathbf{X}(\mathbf{X}^t\mathbf{X})^{-1}\mathbf{X}^t\\
&=\sigma^2_e\mathbf{H},
(\#eq:predls8)
\end{align}
the second line following from \@ref(eq:H).

Finally, for the errors, note that $\mathbf{Y}^{*}$ and $\widehat{\mathbf{Y}}^{*}$ are independent, because the latter depends on the training sample alone. Hence,
\begin{align}
\text{Cov}[\mathbf{Y}^{*} - \widehat {\mathbf{Y}}^{*}] &= \text{Cov}[\mathbf{Y}^{*}] +Cov[\widehat {\mathbf{Y}}^{*}] ~~\mbox{(notice the $+$)}\\
&=\sigma^2_e\mathbf{I}_n+\sigma^2_e\mathbf{H}\\
&=\sigma^2_e(\mathbf{I}_n+\mathbf{H}).
(\#eq:predls9)
\end{align}

Now, 
\begin{align}
n \cdot \text{ERR}_{\text{in}} &= \mathbb{E} [\|\mathbf{Y}^{*} - \widehat {\mathbf{Y}}^{*}\|^2]\\
&=\|\mathbb{E} \mathbf{Y}^{*} - \mathbb{E} \widehat {\mathbf{Y}}^{*} \|^2+ \text{tr}(\text{Cov}[\mathbf{Y}^{*} - \widehat {\mathbf{Y}}^{*}])\\
&=\text{tr} (\sigma^2_e(\mathbf{I}_n+\mathbf{H}))\\
&=\sigma^2_e(n+ \text{tr}(\mathbf{H})).
(\#eq:predls10)
\end{align}
The third line follows from \@ref(eq:predls9) and \@ref(eq:predls5), and the second from the following result: for any random vector $\mathbf{Z}$
\begin{align}
\mathbb{E} [\|\mathbf{Z} \|^2] &= \mathbb{E} [Z_1^2+\cdots+Z_m^2]\\
&= \mathbb{E}[Z_1^2]+\cdots+\mathbb{E}[Z_m^2] \\
&= \mathbb{E}[Z_1]^2+\text{Var}[Z_1]^2+\cdots+\mathbb{E} [Z_m]^2+ \text{Var}[Z_m]^2\\
&= \|\mathbb{E} \mathbf{Z}\|^2 + \text{tr}(\text{Cov}[\mathbf{Z}]),
(\#eq:cov8)
\end{align}
where the trace of a matrix is the sum of the diagonals, which in the case of a
covariance matrix are the variances.

For the trace, recall that $\mathbf{X}$ is $n\times(p+1)$, so that
\begin{align}
\text{tr}(\mathbf{H}) &= \text{tr}(\mathbf{X}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}') \\
&= \text{tr}((\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{X}) \\
&= \text{tr}(\mathbf{I}_{p+1})=p+1.
(\#eq:predls11)
\end{align}
Putting that answer in \@ref(eq:predls10) we obtain
\begin{equation}
\text{ERR}_{\text{in}} = \sigma^2_e + \sigma^2_e~{p+1\over n}.
(\#eq:predls12)
\end{equation}

This expected in-sample error is a simple function of three quantities. We will use it as a benchmark. The goal in the rest
of this section will be to find, if possible, predictors that have lower in-sample error.

There's not much we can do about $\sigma^2_e$,
since it is the inherent variance of the observations. Taking a bigger training sample will decrease the error,
as one would expect. The one part we can work with is the $p$, that is, try to reduce $p$ by eliminating some of the 
explanatory variables. Will that strategy work? It is the subject of the next chapter.


## Practical issues

### Using R
### Interpret LS coefficients
### Handle categorical variables
### Outliers and rank deficiency