[["linear-model-chapter.html", "Chapter 2 Linear Regression 2.1 Good predictions: Squared error loss and in-sample error 2.2 Matrices and least-squares estimates 2.3 Regression inference 2.4 In-sample prediction 2.5 Geometric interpretation 2.6 Practical issues", " Chapter 2 Linear Regression To ease into machine learning, we start with regular linear models. There is one dependent variable, the \\(y\\), and \\(p\\) explanatory variables, the \\(x\\)’s. The data, or training sample, consists of \\(n\\) independent observations: \\[ (y_1,\\mathbf{x}_1), (y_2,\\mathbf{x}_2),\\ldots,(y_n,\\mathbf{x}_n). \\] For individual \\(i\\), \\(y_i\\) is the value of the one-dimensional dependent variable, and \\(\\mathbf{x}_i = (x_{i1}, x_{i2}, \\cdots, x_{ip})^t\\) is the \\(p\\times 1\\) vector of values for the explanatory variables. Generally, the \\(y_i\\)’s are continuous, but the \\(x_{ij}\\)’s can be anything numerical, e.g., 0-1 indicator variables, or functions of another variable (e.g., \\(x,x^2,x^3\\)). The linear model is \\[\\begin{equation} y_i = \\beta_0+\\beta_1 x_{i1}+\\cdots+\\beta_p x_{ip}+e_i. \\tag{2.1} \\end{equation}\\] The \\(\\beta_j\\)’s are parameters, usually unknown and to be estimated. The \\(e_i\\)’s are the errors or residuals. We will assume that The \\(e_i\\)’s are independent (of each other, and of the \\(\\mathbf{x}_i\\)’s); \\(\\mathbb{E} [e_i]=0\\) for each \\(i\\); \\(\\text{Var}[e_i] = \\sigma^2_e\\) for each \\(i\\). There is also a good chance we will assume they are normally distributed. From STAT424 and 425 (or other courses), you know what to do now: estimate the \\(\\beta_j\\)’s and \\(\\sigma^2_e\\), decide which \\(\\beta_j\\)’s are significant, do \\(F\\)-tests, look for outliers and other violations of the assumptions, etc. Here, we may do much of that, but with the goal of prediction. Suppose \\((y^*,\\mathbf{x}^*)\\) is a new point, satisfying the same model and assumptions as above (in particular, being independent of the observed \\(\\mathbf{x}_i\\)’s). Once we have the estimates of the \\(\\beta_j\\)’s (based on the observed data), we predict \\(y^*\\) from \\(\\mathbf{x}^*\\) by \\[\\begin{equation} \\widehat{y}^* = \\widehat{\\beta}_0+\\widehat\\beta_1 x^*_1+\\cdots+\\widehat\\beta_p x^{*}_p. \\tag{2.2} \\end{equation}\\] The prediction is good if \\(\\widehat{y}^{*}\\) is close to \\(y^{*}\\). We do not know \\(y^{*}\\), but we can hope. But the key point is The estimates of the parameters are good if they give good predictions. We don’t care if the \\(\\widehat\\beta_j\\)’s are close to the \\(\\beta_j\\)’s; we don’t care about unbiasedness or minimum variance or significance. We just care whether we get good predictions. 2.1 Good predictions: Squared error loss and in-sample error We want the predictions to be close to the actual (unobserved) value of the dependent variable, that is, we want \\(\\widehat{y}^{*}\\) close to \\(y^{*}\\). One way to measure closeness is by using squared error: \\[ (y^{*}-\\widehat{y}^{*})^2. \\] Because we do not know \\(y^{*}\\) (yet), we might look at the expected value instead: \\[ E[(Y^{*}-\\widehat{Y}^{*})^2]. \\] But what is that the expected value over? Certainly \\(Y^{*}\\), but the \\(Y_i\\)’s and \\(\\mathbf{X}_i\\)’s in the sample, as well as the \\(\\mathbf{X}^{*}\\), could all be considered random. There is no universal answer, but for our purposes here we will assume that the all features \\(\\mathbf{X}\\) are fixed, and all the \\(Y_i\\)’s are random, i.e., \\[\\begin{equation} E[(Y^{*}-\\widehat Y^{*})^2 \\mid \\mathbf{X}_1=\\mathbf{x}_1,\\ldots,\\mathbf{X}_n=\\mathbf{x}_n,\\mathbf{X}^{*}=\\mathbf{x}^{*}]. \\tag{2.3} \\end{equation}\\] But typically you are creating a predictor for many new \\(x\\)’s, and likely you do not know what they will be. (You don’t know what the next 1000 e-mails you get will be.) A reasonable approach is to assume the new \\(\\mathbf{x}\\)’s will look much like the old ones, hence you would look at the errors for \\(n\\) new \\(\\mathbf{x}_i\\)’s being the same as the old ones. Thus we would have \\(n\\) new cases, \\((y_i^{*},\\mathbf{x}_i^{*})\\), but where \\(\\mathbf{X}_i^{*}=\\mathbf{x}_i\\). The \\(n\\) expected errors are averaged, to obtain what is called the in-sample error: \\[\\begin{equation} \\text{ERR}_{\\text{in}} = \\frac{1}{n} \\sum_{i=1}^n E[(Y_i^{*}-\\widehat Y_i^{*})^2~|~\\mathbf{X}_1=\\mathbf{x}_1,\\ldots,\\mathbf{X}_n=\\mathbf{x}_n,\\mathbf{X}_i^{*}=\\mathbf{x}_i^{*}]. \\tag{2.4} \\end{equation}\\] In particular situations, you may have a more precise knowledge of what the new \\(x\\)’s would be. By all means, use those values. We will drop the conditional part of the notation for simplicity. 2.2 Matrices and least-squares estimates Ultimately we want to find estimates of the parameters that yield a low _{}. We’ll start with the least squares estimate, then translate things to matrices. The estimates of the \\(\\beta_j\\)’s depends on just the training sample. The least squares estimate of the parameters are the \\(\\beta_j\\)’s that minimize the objective function \\[\\begin{equation} \\text{RSS}(\\beta_0,\\ldots,\\beta_p) = \\sum_{i=1}^n (y_i-\\beta_0- \\beta_1 x_{i1}-\\cdots- \\beta_p x_{ip})^2. \\tag{2.5} \\end{equation}\\] The function is a nice convex function in the \\(\\beta_j\\)’s, so setting the derivatives equal to zero and solving will yield the minimum. The derivatives are \\[\\begin{align} {\\partial\\over\\partial \\beta_0} \\text{RSS} (\\beta_0,\\ldots,\\beta_p) &amp;= -2~\\sum_{i=1}^N (y_i-\\beta_0-\\beta_1 x_{i1}-\\cdots-\\beta_p x_{ip});\\\\ {\\partial\\over\\partial b_j} \\text{RSS} (\\beta_0,\\ldots,\\beta_p) &amp;= -2~\\sum_{i=1}^N x_{ij}(y_i-\\beta_0-\\beta_1 x_{i1}-\\cdots-\\beta_p x_{ip}), \\quad j\\ge1. \\tag{2.6} \\end{align}\\] Write the equations in matrix form, starting with \\[\\begin{equation} \\begin{pmatrix} y_1-\\beta_0-\\beta_1 x_{11}-\\cdots-\\beta_p x_{1p}\\\\ y_2-\\beta_0-\\beta_1 x_{21}-\\cdots-\\beta_p x_{2p}\\\\ \\vdots\\\\ y_N-\\beta_0-\\beta_1 x_{N1}-\\cdots-\\beta_p x_{Np} \\end{pmatrix} = \\begin{pmatrix} y_1\\\\y_2\\\\\\vdots\\\\y_N \\end{pmatrix} - \\begin{pmatrix} 1&amp;x_{11}&amp;x_{12}&amp;\\cdots&amp;x_{1p}\\\\ 1&amp;x_{21}&amp;x_{22}&amp;\\cdots&amp;x_{2p}\\\\ \\vdots&amp;\\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots\\\\ 1&amp;x_{N1}&amp;x_{N2}&amp;\\cdots&amp;x_{Np} \\end{pmatrix} \\begin{pmatrix} \\beta_0\\\\\\beta_1\\\\\\vdots\\\\\\beta_p \\end{pmatrix} \\tag{2.7} \\end{equation}\\] which is equal to \\(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\). The \\(n\\)-by-\\((p+1)\\) matrix \\(\\mathbf{X}\\) is the so-called design matrix. Take the two summations in equations (2.6) (without the \\(-2\\)’s) and set to 0 to get \\[\\begin{align} \\begin{pmatrix}1&amp;1&amp;\\cdots&amp;1\\end{pmatrix}(\\mathbf{y}-\\mathbf{x}\\boldsymbol{\\beta})&amp;=0;\\\\ \\begin{pmatrix}x_{1j}&amp;x_{2j}&amp;\\cdots&amp;x_{Nj}\\end{pmatrix}(\\mathbf{y}-\\mathbf{x}\\boldsymbol{\\beta})&amp;=0, \\quad j\\ge 1. \\tag{2.8} \\end{align}\\] Note that the row vectors in (2.8) on the left are the \\((p+1)\\) columns of \\(\\mathbf{X}\\), yielding \\[\\begin{equation} \\mathbf{X}^t(\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\beta}) = 0. \\tag{2.9} \\end{equation}\\] That equation is easily solved: \\[\\begin{equation} \\mathbf{X}^t\\mathbf{y} = \\mathbf{X}^t\\mathbf{x}\\boldsymbol{\\beta}~~\\Rightarrow~~\\boldsymbol{\\beta} = (\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\mathbf{y}, \\tag{2.10} \\end{equation}\\] at least if \\(\\mathbf{X}&#39;\\mathbf{X}\\) is invertible. If it is not invertible, then there will be many solutions. In practice, one can always eliminate some (appropriate) columns of \\(\\mathbf{X}\\) to obtain invertibility. Generalized inverses are available, too. Summary. In the linear model \\[\\begin{equation} \\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{e}, \\tag{2.11} \\end{equation}\\] where \\[\\begin{equation} \\boldsymbol{\\beta} = \\begin{pmatrix} \\beta_0\\\\\\beta_1\\\\\\vdots\\\\\\beta_p \\end{pmatrix} ~~\\text{and}~~ \\mathbf{e} = \\begin{pmatrix} e_1\\\\e_2\\\\\\vdots\\\\e_N \\end{pmatrix}, \\tag{2.12} \\end{equation}\\] the least squares estimate of \\(\\boldsymbol{\\beta}\\), assuming \\(\\mathbf{X}^t\\mathbf{X}\\) is invertible, is \\[\\begin{equation} \\widehat{\\boldsymbol{\\beta}}_{LS} = (\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\mathbf{y}. \\tag{2.13} \\end{equation}\\] 2.3 Regression inference 2.4 In-sample prediction When considering the in-sample error for the linear model, we have the same model for the training sample and the new sample: \\[\\begin{equation} \\mathbf{Y}=\\mathbf{X}\\boldsymbol{\\beta}+\\mathbf{e}~~\\mbox{and}~~\\mathbf{Y}^{*} = \\mathbf{X}\\boldsymbol{\\beta}+\\mathbf{e}^{*}. \\tag{2.14} \\end{equation}\\] The \\(e_i\\)’s and \\(e^{*}_i\\)’s are independent with mean 0 and variance \\(\\sigma^2_e\\). If we use the least-squares estimate of \\(\\boldsymbol{\\beta}\\) in the prediction, we have \\[\\begin{equation} \\widehat{\\mathbf{Y}}^{*} = \\mathbf{X}\\widehat{\\boldsymbol{\\beta}}_{LS} = \\mathbf{X}(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;\\mathbf{Y} = \\mathbf{H}\\mathbf{Y}, \\tag{2.15} \\end{equation}\\] where \\(\\mathbf{H}\\) is the “hat” matrix, \\[\\begin{equation} \\mathbf{H} = \\mathbf{X}(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;. \\tag{2.16} \\end{equation}\\] Note that this matrix is symmetric and idempotent, which means that \\[\\begin{equation} \\mathbf{H}^t = \\mathbf{H}, \\quad \\mathbf{H}\\mathbf{H}=\\mathbf{H}. \\tag{2.17} \\end{equation}\\] The errors in prediction are the \\(Y_i^{*} - \\widehat Y_i^{*}\\). Before getting to the \\(\\text{ERR}_{\\text{in}}\\), consider the mean and covariance’s of these errors. First, \\[\\begin{equation} E[\\mathbf{Y}] = E[\\mathbf{X}\\boldsymbol{\\beta}+\\mathbf{e}] = \\mathbf{X}\\boldsymbol{\\beta},~~E[\\mathbf{Y}^{*}] = E[\\mathbf{X}\\boldsymbol{\\beta}+\\mathbf{e}^{*}] = \\mathbf{X}\\boldsymbol{\\beta}, \\tag{2.18} \\end{equation}\\] because the expected values of the \\(e\\)’s are all 0 and we are assuming \\(\\mathbf{X}\\) is fixed, and \\[\\begin{align} E[\\widehat {\\mathbf{Y}}^{*}] &amp;= E[\\mathbf{X}(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;\\mathbf{Y}]\\\\ &amp;= \\mathbf{X}(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;E[\\mathbf{Y}]\\\\ &amp;= \\mathbf{X}(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;\\mathbf{X}\\boldsymbol{\\beta}\\\\ &amp;= \\mathbf{X}\\boldsymbol{\\beta}, \\tag{2.19} \\end{align}\\] because the \\(\\mathbf{X}&#39;\\mathbf{X}\\)’s cancel. Thus, \\[\\begin{equation} \\mathbb{E}[\\mathbf{Y}^{*} - \\widehat {\\mathbf{Y}}^{*}] = \\mathbf{0}_n~~\\mbox{(the $n\\times 1$ vector of 0&#39;s)}. \\tag{2.20} \\end{equation}\\] This zero means that the errors are unbiased. They may be big or small, but on average right on the nose. Unbiasedness is ok, but it is really more important to be close. Next, the covariance matrices: \\[\\begin{equation} \\text{Cov}[\\mathbf{Y}] = \\text{Cov}[\\mathbf{X}\\boldsymbol{\\beta}+\\mathbf{e}] = \\text{Cov}[\\mathbf{e}] = \\sigma_e^2\\mathbf{I}_n~~\\mbox{(the $n\\times n$ identity matrix)}, \\tag{2.21} \\end{equation}\\] because the \\(e_i\\)’s are independent, hence have zero covariance, and all have variance \\(\\sigma^2_e\\). Similarly, \\[\\begin{equation} \\text{Cov}[\\mathbf{Y}^{*}] = \\sigma_e^2\\mathbf{I}_n. \\tag{2.22} \\end{equation}\\] Less similar, \\[\\begin{align} \\text{Cov}[\\widehat {\\mathbf{Y}}^{*}] &amp;= \\text{Cov}[\\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\mathbf{Y}]\\\\ &amp;=\\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t \\text{Cov}[\\mathbf{Y}] \\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\\\ &amp;=\\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t \\sigma^2_e\\mathbf{I}_n \\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\\\ &amp;=\\sigma^2_e\\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\\\ &amp;=\\sigma^2_e\\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\\\ &amp;=\\sigma^2_e\\mathbf{H}, \\tag{2.23} \\end{align}\\] the second line following from (2.16). Finally, for the errors, note that \\(\\mathbf{Y}^{*}\\) and \\(\\widehat{\\mathbf{Y}}^{*}\\) are independent, because the latter depends on the training sample alone. Hence, \\[\\begin{align} \\text{Cov}[\\mathbf{Y}^{*} - \\widehat {\\mathbf{Y}}^{*}] &amp;= \\text{Cov}[\\mathbf{Y}^{*}] +Cov[\\widehat {\\mathbf{Y}}^{*}] ~~\\mbox{(notice the $+$)}\\\\ &amp;=\\sigma^2_e\\mathbf{I}_n+\\sigma^2_e\\mathbf{H}\\\\ &amp;=\\sigma^2_e(\\mathbf{I}_n+\\mathbf{H}). \\tag{2.24} \\end{align}\\] Now, \\[\\begin{align} n \\cdot \\text{ERR}_{\\text{in}} &amp;= \\mathbb{E} [\\|\\mathbf{Y}^{*} - \\widehat {\\mathbf{Y}}^{*}\\|^2]\\\\ &amp;=\\|\\mathbb{E} \\mathbf{Y}^{*} - \\mathbb{E} \\widehat {\\mathbf{Y}}^{*} \\|^2+ \\text{tr}(\\text{Cov}[\\mathbf{Y}^{*} - \\widehat {\\mathbf{Y}}^{*}])\\\\ &amp;=\\text{tr} (\\sigma^2_e(\\mathbf{I}_n+\\mathbf{H}))\\\\ &amp;=\\sigma^2_e(n+ \\text{tr}(\\mathbf{H})). \\tag{2.25} \\end{align}\\] The third line follows from (2.24) and (2.20), and the second from the following result: for any random vector \\(\\mathbf{Z}\\) \\[\\begin{align} \\mathbb{E} [\\|\\mathbf{Z} \\|^2] &amp;= \\mathbb{E} [Z_1^2+\\cdots+Z_m^2]\\\\ &amp;= \\mathbb{E}[Z_1^2]+\\cdots+\\mathbb{E}[Z_m^2] \\\\ &amp;= \\mathbb{E}[Z_1]^2+\\text{Var}[Z_1]^2+\\cdots+\\mathbb{E} [Z_K]^2+ \\text{Var}[Z_m]^2\\\\ &amp;= \\|\\mathbb{E} \\mathbf{Z}\\|^2 + \\text{tr}(\\text{Cov}[\\mathbf{Z}]), \\tag{2.26} \\end{align}\\] where the trace of a matrix is the sum of the diagonals, which in the case of a covariance matrix are the variances. For the trace, recall that \\(\\mathbf{X}\\) is \\(n\\times(p+1)\\), so that \\[\\begin{align} \\text{tr}(\\mathbf{H}) &amp;= \\text{tr}(\\mathbf{X}(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;) \\\\ &amp;= \\text{tr}((\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;\\mathbf{X}) \\\\ &amp;= \\text{tr}(\\mathbf{I}_{p+1})=p+1. \\tag{2.27} \\end{align}\\] Putting that answer in (2.25) we obtain \\[\\begin{equation} ERR_{in} = \\sigma^2_e + \\sigma^2_e~{p+1\\over N}. \\tag{2.28} \\end{equation}\\] This expected in-sample error is a simple function of three quantities. We will use it as a benchmark. The goal in the rest of this section will be to find, if possible, predictors that have lower in-sample error. There’s not much we can do about \\(\\sigma^2_e\\), since it is the inherent variance of the observations. Taking a bigger training sample will decrease the error, as one would expect. The one part we can work with is the \\(p\\), that is, try to reduce \\(p\\) by eliminating some of the explanatory variables. Will that strategy work? It is the subject of the next subsection. 2.5 Geometric interpretation 2.5.1 Basic concepts in vector spaces 2.5.2 LS and projection 2.6 Practical issues 2.6.1 Using R 2.6.2 Interpret LS coefficients 2.6.3 Handle categorical variables 2.6.4 Outliers and rank deficiency "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
